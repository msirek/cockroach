// Code generated by execgen; DO NOT EDIT.
// Copyright 2021 The Cockroach Authors.
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

package colexecjoin

import "github.com/cockroachdb/cockroach/pkg/col/coldata"

const _ = "template_collectProbeOuterLJ"

const _ = "template_collectProbeNoOuterLJ"

// This code snippet collects the "matches" for LEFT ANTI and EXCEPT ALL joins.
// "Matches" are in quotes because we're actually interested in non-matches
// from the left side.
const _ = "template_collectLeftAntiLJ"

// collectRightSemiAnti processes all matches for right semi/anti joins. Note
// that during the probing phase we do not emit any output for these joins and
// are simply tracking whether build rows had a match. The output will be
// populated when in ljEmittingRight state.
func collectRightSemiAntiLJ(lj *lookupJoiner, batchSize int) {
	// Early bounds checks.
	// Capture the slice in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	_ = HeadIDs[batchSize-1]
	for i := 0; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]
		for currentID != 0 {
			lj.probeState.buildRowMatched[currentID-1] = true
			currentID = lj.ht.Same[currentID]
		}
	}
}

const _ = "template_distinctCollectProbeOuterLJ"

const _ = "template_distinctCollectProbeNoOuterLJ"

// collect prepares the buildIdx and probeIdx arrays where the buildIdx and
// probeIdx at each index are joined to make an output row. The total number of
// resulting rows is returned.
func (lj *lookupJoiner) collect(batch coldata.Batch, batchSize int, sel []int) int {
	nResults := 0

	if lj.spec.JoinType.IsRightSemiOrRightAnti() {
		collectRightSemiAntiLJ(lj, batchSize)
		return 0
	}

	if lj.spec.JoinType.IsLeftOuterOrFullOuter() {
		if sel != nil {
			nResults = collectProbeOuterLJ_true(lj, batchSize, nResults, batch, sel)
		} else {
			nResults = collectProbeOuterLJ_false(lj, batchSize, nResults, batch, sel)
		}
	} else {
		if sel != nil {
			if lj.spec.JoinType.IsLeftAntiOrExceptAll() {
				nResults = collectLeftAntiLJ_true(lj, batchSize, nResults, batch, sel)
			} else {
				nResults = collectProbeNoOuterLJ_true(lj, batchSize, nResults, batch, sel)
			}
		} else {
			if lj.spec.JoinType.IsLeftAntiOrExceptAll() {
				nResults = collectLeftAntiLJ_false(lj, batchSize, nResults, batch, sel)
			} else {
				nResults = collectProbeNoOuterLJ_false(lj, batchSize, nResults, batch, sel)
			}
		}
	}

	return nResults
}

// distinctCollect prepares the batch with the joined output columns where the build
// row index for each probe row is given in the GroupID slice. This function
// requires assumes a N-1 hash join.
func (lj *lookupJoiner) distinctCollect(batch coldata.Batch, batchSize int, sel []int) int {
	nResults := 0

	if lj.spec.JoinType.IsRightSemiOrRightAnti() {
		collectRightSemiAntiLJ(lj, batchSize)
		return 0
	}

	if lj.spec.JoinType.IsLeftOuterOrFullOuter() {
		nResults = batchSize

		if sel != nil {
			distinctCollectProbeOuterLJ_true(lj, batchSize, sel)
		} else {
			distinctCollectProbeOuterLJ_false(lj, batchSize, sel)
		}
	} else {
		if sel != nil {
			if lj.spec.JoinType.IsLeftAntiOrExceptAll() {
				// For LEFT ANTI and EXCEPT ALL joins we don't care whether the build
				// (right) side was distinct, so we only have single variation of COLLECT
				// method.
				nResults = collectLeftAntiLJ_true(lj, batchSize, nResults, batch, sel)
			} else {
				nResults = distinctCollectProbeNoOuterLJ_true(lj, batchSize, nResults, sel)
			}
		} else {
			if lj.spec.JoinType.IsLeftAntiOrExceptAll() {
				// For LEFT ANTI and EXCEPT ALL joins we don't care whether the build
				// (right) side was distinct, so we only have single variation of COLLECT
				// method.
				nResults = collectLeftAntiLJ_false(lj, batchSize, nResults, batch, sel)
			} else {
				nResults = distinctCollectProbeNoOuterLJ_false(lj, batchSize, nResults, sel)
			}
		}
	}

	return nResults
}

// execgen:inline
const _ = "template_getIdx"

func collectProbeOuterLJ_true(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	startIdx := lj.probeState.prevBatchResumeIdx
	_ = HeadIDs[startIdx]
	_ = HeadIDs[batchSize-1]
	_ = sel[batchSize-1]
	maxResults := len(lj.probeState.buildIdx)
	buildIdx := lj.probeState.buildIdx
	probeIdx := lj.probeState.probeIdx
	_ = buildIdx[nResults]
	_ = probeIdx[nResults]
	_ = buildIdx[maxResults-1]
	_ = probeIdx[maxResults-1]
	for i := startIdx; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]

		for ; nResults < maxResults; nResults++ {
			rowUnmatched := currentID == 0
			// For some reason, BCE doesn't occur for probeRowUnmatched slice.
			// TODO: figure it out.
			lj.probeState.probeRowUnmatched[nResults] = rowUnmatched
			if rowUnmatched {
				// The row is unmatched, and we set the corresponding buildIdx
				// to zero so that (as long as the build hash table has at least
				// one row) we can copy the values vector without paying
				// attention to probeRowUnmatched.
				//gcassert:bce
				buildIdx[nResults] = 0
			} else {
				//gcassert:bce
				buildIdx[nResults] = int(currentID - 1)
			}
			var pIdx int
			{
				var __retval_0 int
				{
					{
						__retval_0 = sel[i]
					}
				}
				pIdx = __retval_0
			}
			//gcassert:bce
			probeIdx[nResults] = pIdx
			currentID = lj.ht.Same[currentID]
			//gcassert:bce
			HeadIDs[i] = currentID

			if currentID == 0 {
				nResults++
				break
			}
		}

		if nResults == maxResults {
			// We have collected the maximum number of results that fit into the
			// current output batch.
			if currentID != 0 {
				// We haven't finished probing the ith tuple of the current
				// probing batch, so we'll need to resume from the same state.
				lj.probeState.prevBatch = batch
				lj.probeState.prevBatchResumeIdx = i
			} else {
				// We're done probing the ith tuple.
				if i+1 < batchSize {
					// But we're not done probing the batch yet.
					lj.probeState.prevBatch = batch
					lj.probeState.prevBatchResumeIdx = i + 1
				}
			}
			return nResults
		}
	}
	return nResults
}

func collectProbeOuterLJ_false(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	startIdx := lj.probeState.prevBatchResumeIdx
	_ = HeadIDs[startIdx]
	_ = HeadIDs[batchSize-1]
	maxResults := len(lj.probeState.buildIdx)
	buildIdx := lj.probeState.buildIdx
	probeIdx := lj.probeState.probeIdx
	_ = buildIdx[nResults]
	_ = probeIdx[nResults]
	_ = buildIdx[maxResults-1]
	_ = probeIdx[maxResults-1]
	for i := startIdx; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]

		for ; nResults < maxResults; nResults++ {
			rowUnmatched := currentID == 0
			// For some reason, BCE doesn't occur for probeRowUnmatched slice.
			// TODO: figure it out.
			lj.probeState.probeRowUnmatched[nResults] = rowUnmatched
			if rowUnmatched {
				// The row is unmatched, and we set the corresponding buildIdx
				// to zero so that (as long as the build hash table has at least
				// one row) we can copy the values vector without paying
				// attention to probeRowUnmatched.
				//gcassert:bce
				buildIdx[nResults] = 0
			} else {
				//gcassert:bce
				buildIdx[nResults] = int(currentID - 1)
			}
			var pIdx int
			{
				var __retval_0 int
				{
					{
						__retval_0 = i
					}
				}
				pIdx = __retval_0
			}
			//gcassert:bce
			probeIdx[nResults] = pIdx
			currentID = lj.ht.Same[currentID]
			//gcassert:bce
			HeadIDs[i] = currentID

			if currentID == 0 {
				nResults++
				break
			}
		}

		if nResults == maxResults {
			// We have collected the maximum number of results that fit into the
			// current output batch.
			if currentID != 0 {
				// We haven't finished probing the ith tuple of the current
				// probing batch, so we'll need to resume from the same state.
				lj.probeState.prevBatch = batch
				lj.probeState.prevBatchResumeIdx = i
			} else {
				// We're done probing the ith tuple.
				if i+1 < batchSize {
					// But we're not done probing the batch yet.
					lj.probeState.prevBatch = batch
					lj.probeState.prevBatchResumeIdx = i + 1
				}
			}
			return nResults
		}
	}
	return nResults
}

// This code snippet collects the "matches" for LEFT ANTI and EXCEPT ALL joins.
// "Matches" are in quotes because we're actually interested in non-matches
// from the left side.
func collectLeftAntiLJ_true(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slice in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	_ = HeadIDs[batchSize-1]
	_ = sel[batchSize-1]
	for i := 0; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]
		if currentID == 0 {
			// currentID of 0 indicates that ith probing row didn't have a match, so
			// we include it into the output.
			{
				var __retval_0 int
				{
					{
						__retval_0 = sel[i]
					}
				}
				// currentID of 0 indicates that ith probing row didn't have a match, so
				// we include it into the output.
				lj.probeState.probeIdx[nResults] = __retval_0
			}
			nResults++
		}
	}
	return nResults
}

func collectProbeNoOuterLJ_true(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	startIdx := lj.probeState.prevBatchResumeIdx
	_ = HeadIDs[startIdx]
	_ = HeadIDs[batchSize-1]
	_ = sel[batchSize-1]
	maxResults := len(lj.probeState.buildIdx)
	probeIdx := lj.probeState.probeIdx
	_ = probeIdx[nResults]
	_ = probeIdx[maxResults-1]
	for i := startIdx; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]
		for ; currentID != 0 && nResults < maxResults; nResults++ {
			// For some reason, BCE doesn't occur for buildIdx slice.
			// TODO: figure it out.
			lj.probeState.buildIdx[nResults] = int(currentID - 1)
			var pIdx int
			{
				var __retval_0 int
				{
					{
						__retval_0 = sel[i]
					}
				}
				pIdx = __retval_0
			}
			//gcassert:bce
			probeIdx[nResults] = pIdx
			currentID = lj.ht.Same[currentID]
			//gcassert:bce
			HeadIDs[i] = currentID
		}

		if nResults == maxResults {
			// We have collected the maximum number of results that fit into the
			// current output batch.
			if currentID != 0 {
				// We haven't finished probing the ith tuple of the current
				// probing batch, so we'll need to resume from the same state.
				lj.probeState.prevBatch = batch
				lj.probeState.prevBatchResumeIdx = i
			} else {
				// We're done probing the ith tuple.
				if i+1 < batchSize {
					// But we're not done probing the batch yet.
					lj.probeState.prevBatch = batch
					lj.probeState.prevBatchResumeIdx = i + 1
				}
			}
			return nResults
		}
	}
	return nResults
}

// This code snippet collects the "matches" for LEFT ANTI and EXCEPT ALL joins.
// "Matches" are in quotes because we're actually interested in non-matches
// from the left side.
func collectLeftAntiLJ_false(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slice in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	_ = HeadIDs[batchSize-1]
	for i := 0; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]
		if currentID == 0 {
			// currentID of 0 indicates that ith probing row didn't have a match, so
			// we include it into the output.
			{
				var __retval_0 int
				{
					{
						__retval_0 = i
					}
				}
				// currentID of 0 indicates that ith probing row didn't have a match, so
				// we include it into the output.
				lj.probeState.probeIdx[nResults] = __retval_0
			}
			nResults++
		}
	}
	return nResults
}

func collectProbeNoOuterLJ_false(
	lj *lookupJoiner, batchSize int, nResults int, batch coldata.Batch, sel []int) int {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	HeadIDs := lj.ht.ProbeScratch.HeadID
	startIdx := lj.probeState.prevBatchResumeIdx
	_ = HeadIDs[startIdx]
	_ = HeadIDs[batchSize-1]
	maxResults := len(lj.probeState.buildIdx)
	probeIdx := lj.probeState.probeIdx
	_ = probeIdx[nResults]
	_ = probeIdx[maxResults-1]
	for i := startIdx; i < batchSize; i++ {
		//gcassert:bce
		currentID := HeadIDs[i]
		for ; currentID != 0 && nResults < maxResults; nResults++ {
			// For some reason, BCE doesn't occur for buildIdx slice.
			// TODO: figure it out.
			lj.probeState.buildIdx[nResults] = int(currentID - 1)
			var pIdx int
			{
				var __retval_0 int
				{
					{
						__retval_0 = i
					}
				}
				pIdx = __retval_0
			}
			//gcassert:bce
			probeIdx[nResults] = pIdx
			currentID = lj.ht.Same[currentID]
			//gcassert:bce
			HeadIDs[i] = currentID
		}

		if nResults == maxResults {
			// We have collected the maximum number of results that fit into the
			// current output batch.
			if currentID != 0 {
				// We haven't finished probing the ith tuple of the current
				// probing batch, so we'll need to resume from the same state.
				lj.probeState.prevBatch = batch
				lj.probeState.prevBatchResumeIdx = i
			} else {
				// We're done probing the ith tuple.
				if i+1 < batchSize {
					// But we're not done probing the batch yet.
					lj.probeState.prevBatch = batch
					lj.probeState.prevBatchResumeIdx = i + 1
				}
			}
			return nResults
		}
	}
	return nResults
}

func distinctCollectProbeOuterLJ_true(lj *lookupJoiner, batchSize int, sel []int) {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	groupIDs := lj.ht.ProbeScratch.GroupID
	probeRowUnmatched := lj.probeState.probeRowUnmatched
	buildIdx := lj.probeState.buildIdx
	probeIdx := lj.probeState.probeIdx
	_ = groupIDs[batchSize-1]
	_ = probeRowUnmatched[batchSize-1]
	_ = buildIdx[batchSize-1]
	_ = probeIdx[batchSize-1]
	_ = sel[batchSize-1]
	for i := 0; i < batchSize; i++ {
		// Index of keys and outputs in the hash table is calculated as ID - 1.
		//gcassert:bce
		id := groupIDs[i]
		rowUnmatched := id == 0
		//gcassert:bce
		probeRowUnmatched[i] = rowUnmatched
		if rowUnmatched {
			// The row is unmatched, and we set the corresponding buildIdx
			// to zero so that (as long as the build hash table has at least
			// one row) we can copy the values vector without paying
			// attention to probeRowUnmatched.
			//gcassert:bce
			buildIdx[i] = 0
		} else {
			//gcassert:bce
			buildIdx[i] = int(id - 1)
		}
		var pIdx int
		{
			var __retval_0 int
			{
				{
					__retval_0 = sel[i]
				}
			}
			pIdx = __retval_0
		}
		//gcassert:bce
		probeIdx[i] = pIdx
	}
}

func distinctCollectProbeOuterLJ_false(lj *lookupJoiner, batchSize int, sel []int) {
	// Early bounds checks.
	// Capture the slices in order for BCE to occur.
	groupIDs := lj.ht.ProbeScratch.GroupID
	probeRowUnmatched := lj.probeState.probeRowUnmatched
	buildIdx := lj.probeState.buildIdx
	probeIdx := lj.probeState.probeIdx
	_ = groupIDs[batchSize-1]
	_ = probeRowUnmatched[batchSize-1]
	_ = buildIdx[batchSize-1]
	_ = probeIdx[batchSize-1]
	for i := 0; i < batchSize; i++ {
		// Index of keys and outputs in the hash table is calculated as ID - 1.
		//gcassert:bce
		id := groupIDs[i]
		rowUnmatched := id == 0
		//gcassert:bce
		probeRowUnmatched[i] = rowUnmatched
		if rowUnmatched {
			// The row is unmatched, and we set the corresponding buildIdx
			// to zero so that (as long as the build hash table has at least
			// one row) we can copy the values vector without paying
			// attention to probeRowUnmatched.
			//gcassert:bce
			buildIdx[i] = 0
		} else {
			//gcassert:bce
			buildIdx[i] = int(id - 1)
		}
		var pIdx int
		{
			var __retval_0 int
			{
				{
					__retval_0 = i
				}
			}
			pIdx = __retval_0
		}
		//gcassert:bce
		probeIdx[i] = pIdx
	}
}

func distinctCollectProbeNoOuterLJ_true(
	lj *lookupJoiner, batchSize int, nResults int, sel []int) int {
	// Early bounds checks.
	// Capture the slice in order for BCE to occur.
	groupIDs := lj.ht.ProbeScratch.GroupID
	_ = groupIDs[batchSize-1]
	_ = sel[batchSize-1]
	for i := 0; i < batchSize; i++ {
		//gcassert:bce
		id := groupIDs[i]
		if id != 0 {
			// Index of keys and outputs in the hash table is calculated as ID - 1.
			lj.probeState.buildIdx[nResults] = int(id - 1)
			{
				var __retval_0 int
				{
					{
						__retval_0 = sel[i]
					}
				}
				lj.probeState.probeIdx[nResults] = __retval_0
			}
			nResults++
		}
	}
	return nResults
}

func distinctCollectProbeNoOuterLJ_false(
	lj *lookupJoiner, batchSize int, nResults int, sel []int) int {
	// Early bounds checks.
	// Capture the slice in order for BCE to occur.
	groupIDs := lj.ht.ProbeScratch.GroupID
	_ = groupIDs[batchSize-1]
	for i := 0; i < batchSize; i++ {
		//gcassert:bce
		id := groupIDs[i]
		if id != 0 {
			// Index of keys and outputs in the hash table is calculated as ID - 1.
			lj.probeState.buildIdx[nResults] = int(id - 1)
			{
				var __retval_0 int
				{
					{
						__retval_0 = i
					}
				}
				lj.probeState.probeIdx[nResults] = __retval_0
			}
			nResults++
		}
	}
	return nResults
}

// execgen:inline
const _ = "inlined_getIdx_true"

// execgen:inline
const _ = "inlined_getIdx_false"
